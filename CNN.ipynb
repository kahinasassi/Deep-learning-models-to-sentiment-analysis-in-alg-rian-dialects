{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kahinasassi/Deep-learning-models-to-sentiment-analysis-in-alg-rian-dialects/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rljMPD3vubss"
      },
      "source": [
        "# Auteur: SASSI KAHINA ISII\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWTUodMrLg-z"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YORFsl_wulrs"
      },
      "source": [
        "# Importer les librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvsMlUu4Ln4j"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import re\n",
        "from nltk import defaultdict\n",
        "import io\n",
        "import codecs\n",
        "import csv\n",
        "import pickle\n",
        "import nltk\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Bidirectional ,Conv1D, Dropout  ,MaxPooling1D,Flatten\n",
        "from keras.layers import TimeDistributed\n",
        "import keras\n",
        "from keras.optimizers import Adam , SGD, RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWsdqIbEl_wx"
      },
      "source": [
        "!pip install scikit-plot\n",
        "!pip install -q pyyaml h5py  # Required to save models in HDF5 format\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import re\n",
        "from nltk import defaultdict\n",
        "import io\n",
        "from keras.models import Model\n",
        "\n",
        "import codecs\n",
        "import csv\n",
        "import pickle\n",
        "import nltk\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D \n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import scikitplot as skplt\n",
        "import h5py\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Bidirectional ,Conv1D, Dropout  ,MaxPooling1D,Flatten,Concatenate\n",
        "from keras.layers import TimeDistributed\n",
        "import keras\n",
        "from keras.optimizers import Adam , SGD, RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import regularizers\n",
        "from collections import Counter\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Input, Dense, Embedding, Conv1D, MaxPool1D\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUB1MJjgut5o"
      },
      "source": [
        "# métrique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN3sqaqfLrLN"
      },
      "source": [
        "\n",
        "\n",
        "def save_pickle(variable,filename):\n",
        "    with open(filename, 'wb') as handle:\n",
        "         pickle.dump(variable, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "#modified\n",
        "from keras import backend as K\n",
        "\n",
        "def check_units(y_true, y_pred):\n",
        "    if y_pred.shape[1] != 1:\n",
        "        y_pred = y_pred[:,1:2]\n",
        "        y_true = y_true[:,1:2]\n",
        "    \n",
        "    return y_true, y_pred\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    y_true, y_pred = check_units(y_true, y_pred)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    \n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    y_true, y_pred = check_units(y_true, y_pred)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    \n",
        "    return recall\n",
        "\n",
        "def fmeasure(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        \n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        \n",
        "        return precision\n",
        "    \n",
        "    y_true, y_pred = check_units(y_true, y_pred)\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    \n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "#  optimizer\n",
        "sgd = keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
        "rms_prop = keras.optimizers.RMSprop(lr=1e-4)\n",
        "#opt = Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
        "\n",
        "adamax = keras.optimizers.Adamax(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
        "adadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "\n",
        "#********************************************* graph of Accuaracy and loss*******************************\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "# ouvrir un fichier pickle\n",
        "def open_pickle(path):\n",
        "  with open(path, 'rb') as handle: \n",
        "    doc= pickle.load(handle)\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DjOQ66kdSUQ"
      },
      "source": [
        "#         1          Ensemble de données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKa1TMr_eVRC"
      },
      "source": [
        "##Remarque: décommenter 1 seul ensemble de données à tester"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GtV2BXwcHp3"
      },
      "source": [
        "#Mataoui originale: 2classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YKhiBqUcGD7"
      },
      "source": [
        "#data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/mataouiOrg.pickle\")\n",
        "'''\n",
        "data.info()\n",
        "print('**********************************************************************')\n",
        "print(data.shape)\n",
        "print('**********************************************************************')\n",
        "#print(data.label.value_counts())\n",
        "print(\"nbr de val positives = \",len(data.loc[data['label'] == 1].iloc[:,1]))\n",
        "print(\"nbr de val negatives = \",len(data.loc[data['label'] == -1].iloc[:,1]))\n",
        "print(\"nbr de val neutres = \",len(data.loc[data['label'] == 0].iloc[:,1]))\n",
        "print(\"**********************************************************************\")\n",
        "print(\"les valeurs nuls:\\n\", data.isnull().sum())\n",
        "data = pd.DataFrame(data) '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxKjReRmYgpA"
      },
      "source": [
        "#Mataoui avec 3 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWanN5z0YWQS"
      },
      "source": [
        "#data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/dataMataoui.pickle\")\n",
        "'''\n",
        "data.info()\n",
        "print('**********************************************************************')\n",
        "print(data.shape)\n",
        "print('**********************************************************************')\n",
        "#print(data.label.value_counts())\n",
        "print(\"nbr de val positives = \",len(data.loc[data['label'] == 1].iloc[:,1]))\n",
        "print(\"nbr de val negatives = \",len(data.loc[data['label'] == -1].iloc[:,1]))\n",
        "print(\"nbr de val neutres = \",len(data.loc[data['label'] == 0].iloc[:,1]))\n",
        "print(\"**********************************************************************\")\n",
        "print(\"les valeurs nuls:\\n\", data.isnull().sum())\n",
        "data = pd.DataFrame(data) '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7vZ94C1Ylhv"
      },
      "source": [
        "#BrandtDz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUjBgEr6YsFa"
      },
      "source": [
        "#data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/BRANDT.pickle\")\n",
        "'''\n",
        "data.info()\n",
        "print('**********************************************************************')\n",
        "print(data.shape)\n",
        "print('**********************************************************************')\n",
        "#print(data.label.value_counts())\n",
        "print(\"nbr de val positives = \",len(data.loc[data['label'] == 1].iloc[:,1]))\n",
        "print(\"nbr de val negatives = \",len(data.loc[data['label'] == -1].iloc[:,1]))\n",
        "print(\"nbr de val neutres = \",len(data.loc[data['label'] == 0].iloc[:,1]))\n",
        "print(\"**********************************************************************\")\n",
        "print(\"les valeurs nuls:\\n\", data.isnull().sum())\n",
        "data = pd.DataFrame(data) '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0QUUvPGZFqS"
      },
      "source": [
        "#Madar+ YouTube"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo1h-uKKZJNK"
      },
      "source": [
        "#data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/data_mixed.pickle\")\n",
        "'''\n",
        "data.info()\n",
        "print('**********************************************************************')\n",
        "print(data.shape)\n",
        "print('**********************************************************************')\n",
        "#print(data.label.value_counts())\n",
        "print(\"nbr de val positives = \",len(data.loc[data['label'] == 1].iloc[:,1]))\n",
        "print(\"nbr de val negatives = \",len(data.loc[data['label'] == -1].iloc[:,1]))\n",
        "print(\"nbr de val neutres = \",len(data.loc[data['label'] == 0].iloc[:,1]))\n",
        "print(\"**********************************************************************\")\n",
        "print(\"les valeurs nuls:\\n\", data.isnull().sum())\n",
        "data = pd.DataFrame(data)''' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3IPdpTWYL-6"
      },
      "source": [
        "#dataset rassemblé (non équilibrer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqO9jGb3LtWU"
      },
      "source": [
        "data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/finale_data.pickle\")\n",
        "\n",
        "data.info()\n",
        "print('**********************************************************************')\n",
        "print(data.shape)\n",
        "print('**********************************************************************')\n",
        "#print(data.label.value_counts())\n",
        "print(\"nbr de val positives = \",len(data.loc[data['label'] == 1].iloc[:,1]))\n",
        "print(\"nbr de val negatives = \",len(data.loc[data['label'] == -1].iloc[:,1]))\n",
        "print(\"nbr de val neutres = \",len(data.loc[data['label'] == 0].iloc[:,1]))\n",
        "print(\"**********************************************************************\")\n",
        "print(\"les valeurs nuls:\\n\", data.isnull().sum())\n",
        "data = pd.DataFrame(data) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM5WctApc8yQ"
      },
      "source": [
        "#ensemble de données équilibere\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAYoBiOfZRT9"
      },
      "source": [
        "'''\n",
        "data=open_pickle(\"/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/ressources/finale_data.pickle\")\n",
        "\n",
        "data = pd.DataFrame(data) \n",
        "\n",
        "pos = data[data['label'] ==1]\n",
        "neg = data[data['label'] == -1]\n",
        "neu = data[data['label'] == 0]\n",
        "\n",
        "max_len = min(len(pos), len(neg), len(neu))\n",
        "\n",
        "data_ = []\n",
        "for i in range(max_len):\n",
        "  data_ += [{'text' : pos.iloc[i]['text'], 'label' : pos.iloc[i]['label']}]\n",
        "  data_ += [{'text' : neg.iloc[i]['text'], 'label' : neg.iloc[i]['label']}]\n",
        "  data_ += [{'text' : neu.iloc[i]['text'], 'label' : neu.iloc[i]['label']}]\n",
        "\n",
        "data = pd.DataFrame(data_)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1HEJW7TdMnC"
      },
      "source": [
        "#Traitement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMs8bLsfdLd2"
      },
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'],data['label'], train_size=0.90)\n",
        "\n",
        "texte = X_train.append(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtV3Fd1BeOj6"
      },
      "source": [
        "#Encodage du text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goswIs0TL3wf"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count unique words\n",
        "def counter_word(text):\n",
        "    count = Counter()\n",
        "    for i in text.values:\n",
        "        for word in i.split():\n",
        "            count[word] += 1\n",
        "    return count\n",
        "\n",
        "counter = counter_word(texte)\n",
        "num_words = len(counter)# we 'll use it to vocab \n",
        "\n",
        "# Max number of words in a sequence\n",
        "max_length = 2000\n",
        "print('vocab=', len(counter),' and max_length=',max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD666SseL5-J"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "train_padded = pad_sequences( train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "test_padded = pad_sequences( test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode(text):\n",
        "    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])\n",
        "decode(train_sequences[0])\n",
        "\n",
        "#len(valid_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aotQ7vQIeKfO"
      },
      "source": [
        "#Encodage des labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G_oEis_L9wy"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "y_train = to_categorical(y_train, num_classes=3)\n",
        "y_test = to_categorical(y_test, num_classes=3)\n",
        "\n",
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEUj4Z4_IlvF"
      },
      "source": [
        "train_padded.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y65teFPcdo-m"
      },
      "source": [
        "#Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifm_mIY4dwN6"
      },
      "source": [
        "##3 couches de convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwAhgqDylzLq"
      },
      "source": [
        "from keras.layers import Input, Dense, Embedding, Conv1D, MaxPool1D\n",
        "\n",
        "inp = Input(shape=(max_length,)) \n",
        "embedding = Embedding(input_dim=num_words,output_dim=100 )(inp)\n",
        "conv_0 = Conv1D(30 , kernel_size=2, padding='valid', activation='relu')(embedding)\n",
        "conv_1 = Conv1D(30 , kernel_size=3, padding='valid', activation='relu')(embedding)\n",
        "conv_2 = Conv1D(30 , kernel_size=4, padding='valid', activation='relu')(embedding)\n",
        "\n",
        "maxpool_0 = MaxPooling1D(pool_size=2)(conv_0)\n",
        "maxpool_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
        "maxpool_2 = MaxPooling1D(pool_size=2)(conv_2)\n",
        "\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1,maxpool_2])\n",
        "model = Flatten()(concatenated_tensor)\n",
        "model = Dropout(0.3)(model)\n",
        "\n",
        "#model = Dense(20)(model)\n",
        "\n",
        "model = Dense(3, activation='softmax')(model)\n",
        "model = Model(inputs=inp, outputs=model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5w5kLS9d0Vn"
      },
      "source": [
        "##1 couche de convlution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rEMB8RnL_hS"
      },
      "source": [
        "'''\n",
        "model = Sequential() \n",
        "model.add(Embedding(num_words,100,input_shape= (train_padded.shape[1],)))\n",
        "model.add(SpatialDropout1D(0.4))#0.4\n",
        "model.add(Conv1D(filters=20, kernel_size=2, padding='valid', activation='tanh'))#activation='relu'\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "#model.add(Dense(20, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(3,activation='softmax'))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DGIoFfod5gI"
      },
      "source": [
        "#compilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiDzhQYZanhr"
      },
      "source": [
        "#  optimizer\n",
        "sgd = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "rms_prop = keras.optimizers.RMSprop(lr=0.0001)#1e-4\n",
        "opt = Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n",
        "\n",
        "#adam = keras.optimizers.adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
        "adamax = keras.optimizers.Adamax(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
        "adadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.0001)\n",
        "model.compile(optimizer = rms_prop, loss = keras.losses.categorical_crossentropy, metrics = ['accuracy', recall, precision, fmeasure])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRAnwuvsd8Nj"
      },
      "source": [
        "#Entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0AXH_xfMIZ5"
      },
      "source": [
        "history = model.fit(train_padded, y_train, epochs=50, batch_size=100,validation_split=0.10,callbacks=[EarlyStopping(monitor='val_loss',patience=3, min_delta=0.0001)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyr744hxd_CZ"
      },
      "source": [
        "#Evaulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMnObrhMQNj"
      },
      "source": [
        "result = model.evaluate(x=test_padded, y= y_test)\n",
        "print('Accuracy:', result[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_Z2hUK77Iec"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#*********************************************Accuaracy and loss*******************************\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuH9OyVAhcZU"
      },
      "source": [
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyWNhU60I4re"
      },
      "source": [
        "\n",
        "from sklearn.metrics import classification_report\n",
        "Y_pred = model.predict(test_padded)\n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "\n",
        "print(classification_report(Y_true,Y_pred_classes))\n",
        "\n",
        "# confusion matrix\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = model.predict(test_padded)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "# plot the confusion matrix\n",
        "f,ax = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBKBv7eqeFFX"
      },
      "source": [
        "#sauvgarde"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrZ213FLkm7Q"
      },
      "source": [
        "\n",
        "def savemodel(model,name):\n",
        "  model.save('/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/CODE/'+name)\n",
        "def open_model(name):\n",
        "  return keras.models.load_model('/content/drive/My Drive/PROJET FIN ETUDE 2019 2020/CODE/'+name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emZGfi443acN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}